\documentclass[16ptï¼Œletterpaper]{ctexart}
\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in]{geometry}


\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

\makeatletter% since there's an at-sign (@) in the command name
\renewcommand{\@maketitle}{%
  \parindent=0pt% don't indent paragraphs in the title block
  \centering
  {\Large \bfseries\textsc{\@title}}
  \HRule\par%
  \textit{\@author \hfill \@date}
  \par
}
\makeatother% resets the meaning of the at-sign (@)

\title{Statement of Purpose}
\author{Nan Jiang}
\date{29/10/2017}

\begin{document}
  \maketitle% prints the title block
  \thispagestyle{empty}
  \vspace{12pt}


I am currently a graduate student from the School of Computer Science and Engineering, Beihang University. My academic advisor is Prof. Wenge Rong, and my research interests are Deep Learning methods (DL) on Natural Language Processing (NLP). I got my Bachelor's degree in Computer Science and Technology and second Bachelor's degree in Electronic Engineering from Zhejiang University of Technology.


\subsection{My Academic Study and Internship}
My undergraduate study can be divided into two periods. During my freshman and sophomore year, I paid great effort in math, programming courses and conducted class experiments from simple controlled chips to algorithm problems, from crawling data on websites to designing information management system. All efforts are not made for nothing, and I have got all the worthwhile awards in college. In the junior and senior year, I took courses at Beihang University as a exchange student and did experiments under my current professor. It became appealing to me to purchase a graduate degree on DL/NLP, after several months about learning related knowledge and publishing a paper about Sparse Penalty on Autoencoder model.

Afterwards, during my graduate study, I learn diverse knowledge from MILA lab, such as the tutorial website: ``deeplearning.net'' and the summer school videos from 2015 to 2017. Furthermore, I attended two internship to practice these fascinating theories about deep models on real-world challenge, like experimenting with Language Models (LM) at Youdao Department in Netease and modifying Neural Dialog Generation (NRG) models at Search Technology Center Asia (STCA), Microsoft.

To illustrate, Youdao department held a weekly paper sharing program, which benefited me a lot on learning LM and Neural Machine Translation (NMT) variations. Understanding and implementing these algorithms challenged me a lot and was a big headache during these times. With devoted effort for 3 months, I searched through the Github website collecting all the useful codes and wrote down the exact calculation codes using Theano. After several times of inspection on papers and the codes, I was thrilled to witness the correct experimental results. Furthermore, one intuition about parallelized calculation of the Hierarchical Softmax hit my mind, and I deducted the math equation several times, making sure that it is right. Nevertheless, the review comments from WWW 2017 was not that promising: these equations need to be aligned and the description of algorithm should be more precise. So extended efforts are conducted on correcting these mistakes and it was published at IJCAI 2017 conference. Also, the detailed journal paper is currently under review at TNNLS (IEEE Transactions).

Besides, I also got a NLP internship at STCA at Microsoft. In 9 months, my mentor and I have been doing experiments on ranking-based optimization to improve NRG model and writing papers on three different conferences EMNLP 2017, NIPS 2017 and AAAI 2018. These specialized conferences on NLP/ML are highly competitive and our works were not accepted by the chair, but we still gained experience in formalizing our ideas into pages and conducting extensive experiments to validate the theories. Apart from catching the conference's deadline, my mentor and I, together with another intern from Peking University also attended competitions at Kaggle and KDD CUP. For example, in the Quora duplicate question identification competition at Kaggle, I contributed to developing various deep models to extract features that measure different metrics, which would later be ensembled with other sparse statistical features with XGBoost.

\subsection{Why Pursuing PhD Degree?}
There are two options for my life-path: pursuing a PhD degree or finding a job about DL. The answer became clear to me after working in Microsoft as an intern for 9 months and having a detailed conversation on the issues and outlook of Dialog Models with Prof. Aaron Courvile at IJCAI 2017 conference. Doing the daily routine works and applying existing algorithms to current system is easy, yet I want to take the road less travelled, exploring the detailed structures of DL/NLP. Instead of observing the great development of DL models, it would better to get in the game. Besides, people nowadays solely cascade these historical proposed modules to achieve better performance in existing tasks. Yet permutating over different modules is not enough, performance need to be improved and observed results acquire better theoretical explanations.

Devoting all my energy to the development of NLP is challenging yet fascinating, as I believe that the pattern of human language can be revealed by experts and mimicking human conversations will be achieved with specified models. From the first day I got acquainted with DL theories, MILA Lab at Montreal has helped me acquire knowledge though diverse channels, like coding samples, detailed videos and published papers. Learning and working at this advanced scientific lab has been my dream, so I exploit every possibility on improving my knowledge on the fields of ML/DL, to meet up the qualification of PhD candidates in the lab.

\subsection{Planning For Doctoral Study}

\textbf{Learning with Large-Scale Text Data.}  In the past few years, the size of text data has grown drastically and it becomes necessary for conventional algorithms to converge faster and calculate with better efficiency. Programming with Python is currently the most useful one yet the fastest, building up interfaces with CUDA-based algorithms is efficient yet optimal. As I am learning CUDA-C programming language, there will be framework that build exactly on the fundamental language. Therefore, the capacity of deep models can be greatly exploited and the required time for training will be largely diminished. Building such kind of framework require strong coding skills and efforts, devoting to this possible area is time-consuming yet worthwhile, as it would benefit the future researchers and provide them chances to fully experiment their models within the limited time.

\textbf{Possibility of Generative Model on Text.} Despite the traditional supervised and unsupervised tasks, we are at the stage of exploring the effectiveness of generative model. Current advances are based on designing deep modules that fit personal observation, such kind of achievement will not last for long without a more concrete and fundamental theories that cover all the partial explanations into a united one. It is challenging and require complex mathematical theories, and I will devote my time on learning required math courses to present current model with both experimental results and theoretical explanations. As I believe that theories need to be built for generations before we investigate effective pattern for human's conversation.

\textbf{Deeper Insight for Text Models.} As I did internship in companies, I have witnessed that the quality for simple task is greatly needed. To illustrate, the real-world datasets for sentiment analysis is much more complex than the standard dataset in research, and it is hard to learn presumable results with current deep models. Therefore, I will devote my effort on improve these well-defined and important tasks, migrating the gap between the academic models and industrial needs.
\end{document}
